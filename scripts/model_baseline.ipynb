{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [
        {
          "file_id": "1qpGwGYavjA1ysEvsppBch3waeIH5bzGg",
          "timestamp": 1748823640473
        }
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPp3X57ij+cKqVnYIV8hAfk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-KDM-gppfvC",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1748999180391,
          "user_tz": -540,
          "elapsed": 4240,
          "user": {
            "displayName": "melon",
            "userId": "16479606032837208631"
          }
        },
        "outputId": "e9230bae-20b9-400b-f616-d7891a391f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkFxhUKcpj45",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1748999186442,
          "user_tz": -540,
          "elapsed": 6049,
          "user": {
            "displayName": "melon",
            "userId": "16479606032837208631"
          }
        },
        "outputId": "fe0f4c8a-e64a-45e1-eaa8-3b51ddb3c273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V6PeIJAYpmAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1748999188124,
          "user_tz": -540,
          "elapsed": 1657,
          "user": {
            "displayName": "melon",
            "userId": "16479606032837208631"
          }
        },
        "outputId": "551ab3ae-aa35-4202-9146-15811366063f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoTokenizer, ElectraForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "\n",
        "PADDING_TOKEN = 0\n",
        "S_OPEN_TOKEN = 1\n",
        "S_CLOSE_TOKEN = 2\n",
        "do_eval = True\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/ABSA'\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
        "ACD_MODEL_DIR = os.path.join(BASE_DIR, 'saved_model/ACD')\n",
        "ASC_MODEL_DIR = os.path.join(BASE_DIR, 'saved_model/ASC')\n",
        "\n",
        "train_data_path = os.path.join(DATA_DIR, 'train.jsonl')\n",
        "dev_data_path = os.path.join(DATA_DIR, 'dev.jsonl')\n",
        "test_data_path = os.path.join(DATA_DIR, 'test.jsonl')\n",
        "\n",
        "test_category_extraction_model_path = os.path.join(ACD_MODEL_DIR, 'best_model.pt')\n",
        "test_polarity_classification_model_path = os.path.join(ASC_MODEL_DIR, 'best_model.pt')\n",
        "\n",
        "max_len = 192\n",
        "batch_size = 32\n",
        "base_model = 'kykim/electra-kor-base'\n",
        "learning_rate = 3e-6\n",
        "eps = 1e-8\n",
        "num_train_epochs = 20\n",
        "classifier_hidden_size = 768\n",
        "classifier_dropout_prob = 0.1\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "entity_property_pair = [\n",
        "    '세정', '자극', '거품', '향', '가격', '일반', '기타',\n",
        "]\n",
        "\n",
        "tf_id_to_name = ['True', 'False']\n",
        "tf_name_to_id = {name: idx for idx, name in enumerate(tf_id_to_name)}\n",
        "\n",
        "polarity_id_to_name = ['positive', 'negative', 'neutral']\n",
        "polarity_name_to_id = {name: idx for idx, name in enumerate(polarity_id_to_name)}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "special_tokens_dict = {\n",
        "    'additional_special_tokens': [\n",
        "        '&name&', '&affiliation&',\n",
        "        '&social-security-num&',\n",
        "        '&tel-num&', '&card-num&', '&bank-account&',\n",
        "        '&num&', '&online-account&'\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "aueYJZI1pnsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jsonload(fname, encoding=\"utf-8\"):\n",
        "    with open(fname, encoding=encoding) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def jsondump(j, fname):\n",
        "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(j, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def jsonlload(fname, encoding=\"utf-8\"):\n",
        "    json_list = []\n",
        "    with open(fname, encoding=encoding) as f:\n",
        "        for line in f:\n",
        "            json_list.append(json.loads(line))\n",
        "    return json_list\n",
        "\n",
        "def jsonldump(jlist, fname):\n",
        "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in jlist:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "def split_jsonl_file(jsonl_path, output_dir, train_ratio=0.7, dev_ratio=0.15, test_ratio=0.15, seed=42):\n",
        "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "        lines = [json.loads(line) for line in f]\n",
        "\n",
        "    train_data, temp_data = train_test_split(lines, test_size=(1 - train_ratio), random_state=seed)\n",
        "    dev_data, test_data = train_test_split(temp_data, test_size=test_ratio / (dev_ratio + test_ratio), random_state=seed)\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    jsonldump(train_data, os.path.join(output_dir, 'train.jsonl'))\n",
        "    jsonldump(dev_data, os.path.join(output_dir, 'dev.jsonl'))\n",
        "    jsonldump(test_data, os.path.join(output_dir, 'test.jsonl'))\n",
        "\n",
        "    print(f\"데이터 분할 완료: train={len(train_data)}, dev={len(dev_data)}, test={len(test_data)}\")"
      ],
      "metadata": {
        "id": "BzzYmz0DpxVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_absa_format(data):\n",
        "    converted = []\n",
        "    for item in data:\n",
        "        sentence = item[\"text\"]\n",
        "        annos = []\n",
        "        for entity in item.get(\"entities\", []):\n",
        "            label = entity[\"label\"]\n",
        "            start = entity[\"start_offset\"]\n",
        "            end = entity[\"end_offset\"]\n",
        "            word = sentence[start:end]\n",
        "\n",
        "            try:\n",
        "                aspect, sentiment_ko = label.split(\"-\")\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "            if aspect not in entity_property_pair:\n",
        "                continue\n",
        "\n",
        "            sentiment_map = {\"긍정\": \"positive\", \"부정\": \"negative\", \"중립\": \"neutral\"}\n",
        "            polarity = sentiment_map.get(sentiment_ko.strip())\n",
        "            if polarity is None:\n",
        "                continue\n",
        "\n",
        "            annos.append([aspect, [word, start, end], polarity])\n",
        "        converted.append({\n",
        "            \"sentence_form\": sentence,\n",
        "            \"annotation\": annos\n",
        "        })\n",
        "    return converted\n",
        "\n",
        "\n",
        "raw_data = jsonlload('/content/drive/MyDrive/ABSA/data/rawdata_final.jsonl')\n",
        "converted_data = convert_to_absa_format(raw_data)\n",
        "converted_data_path = os.path.join(DATA_DIR, 'converted_rawdata_final.jsonl')\n",
        "jsonldump(converted_data, converted_data_path)\n",
        "# jsonlload('/content/drive/MyDrive/ABSA/data/converted_shampoo.jsonl')"
      ],
      "metadata": {
        "id": "RDJDHwc3WY7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "# from pykospacing import Spacing\n",
        "# spacing = Spacing()\n",
        "\n",
        "# def spacing_cpu_only(text):\n",
        "#     return spacing(text)\n",
        "\n",
        "def clean_review(text):\n",
        "    allowed_punctuations = \"!?.,%+=~&()\"\n",
        "    pattern = rf\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9\\s{re.escape(allowed_punctuations)}]\"\n",
        "    return re.sub(pattern, ' ', text).strip()\n",
        "\n",
        "def del_bracket(text):\n",
        "    while re.search(r'\\([^()]*\\)', text):\n",
        "        text = re.sub(r'\\([^()]*\\)', ' ', text)\n",
        "    return text.replace(\"(\", \" \").replace(\")\", \" \").strip()\n",
        "\n",
        "dupchars_pattern = re.compile(r'(.)\\1{2,}')\n",
        "dupsymbols_pattern = re.compile(r'([!?~%+=&])\\1{1,}')\n",
        "doublespace_pattern = re.compile(r'\\s+')\n",
        "\n",
        "def contract_dupchars(text, n=3):\n",
        "    if n > 0:\n",
        "        text = dupchars_pattern.sub(r'\\1' * n, text)\n",
        "    text = dupsymbols_pattern.sub(r'\\1', text)\n",
        "    return doublespace_pattern.sub(' ', text).strip()\n",
        "\n",
        "def del_sponsored(text):\n",
        "    pattern = re.compile(r'''\n",
        "        (판매자(에게|로부터)|업체로부터|본\\s상품\\s후기는).{0,70}?\n",
        "        (후기(입니다|입니다타|에요|입니\\s?다)?|\n",
        "         리뷰(입니다|했습니다|적었습니다|하였습니다|입니다요)?|\n",
        "         작성하였습니다|기남겨요|기랍니다|흐기입니다|전달합니다|올립니다|것\\s?입니다)\n",
        "        [!.~\\s]{0,2}\n",
        "    ''', flags=re.VERBOSE)\n",
        "    return pattern.sub(' ', text).strip()\n",
        "\n",
        "# def truncate_and_spacing(text):\n",
        "#     if len(text) > 200:\n",
        "#         text = text[-200:]\n",
        "#     return spacing_cpu_only(text)\n",
        "\n",
        "def preprocessing(form):\n",
        "    form = clean_review(form)\n",
        "    form = del_bracket(form)\n",
        "    form = contract_dupchars(form)\n",
        "    form = del_sponsored(form)\n",
        "    # form = truncate_and_spacing(form)\n",
        "    return form"
      ],
      "metadata": {
        "id": "JnGX2uTNGcH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoConfig\n",
        "\n",
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, hidden_states, mask):\n",
        "        scores = self.attention(hidden_states).squeeze(-1)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        pooled = torch.sum(hidden_states * weights.unsqueeze(-1), dim=1)\n",
        "        return pooled\n",
        "\n",
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, hidden_size, num_labels, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout1 = nn.Dropout(dropout_prob)\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.act = nn.Tanh()\n",
        "        self.norm = nn.LayerNorm(hidden_size)\n",
        "        self.dropout2 = nn.Dropout(dropout_prob)\n",
        "        self.fc2 = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.dropout2(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "class ElectraBaseClassifier(nn.Module):\n",
        "    def __init__(self, model_name_or_path, num_labels, tokenizer_len=None, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        config = AutoConfig.from_pretrained(model_name_or_path)\n",
        "        self.backbone = AutoModel.from_pretrained(model_name_or_path, config=config)\n",
        "\n",
        "        if tokenizer_len is not None:\n",
        "            self.backbone.resize_token_embeddings(tokenizer_len)\n",
        "\n",
        "        self.attn_pool = AttentionPooling(config.hidden_size)\n",
        "        self.classifier = SimpleClassifier(config.hidden_size, num_labels, dropout_prob)\n",
        "        self.loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "\n",
        "        pooled_output = self.attn_pool(hidden_states, attention_mask)\n",
        "\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fn(logits, labels)\n",
        "            return loss, logits\n",
        "        else:\n",
        "            return None, logits"
      ],
      "metadata": {
        "id": "Lux_4Z-op29z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize_and_align_labels(tokenizer, form, annotations, max_len):\n",
        "    entity_data = {'input_ids': [], 'attention_mask': [], 'label': []}\n",
        "    polarity_data = {'input_ids': [], 'attention_mask': [], 'label': []}\n",
        "\n",
        "    if not form or not isinstance(form, str):\n",
        "        return entity_data, polarity_data\n",
        "\n",
        "    for pair in entity_property_pair:\n",
        "        matched = False\n",
        "\n",
        "        encoded = tokenizer(\n",
        "            form,\n",
        "            pair,\n",
        "            padding='max_length',\n",
        "            max_length=max_len,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoded['input_ids'][0].tolist()\n",
        "        attention_mask = encoded['attention_mask'][0].tolist()\n",
        "\n",
        "        for annotation in annotations:\n",
        "            if len(annotation) < 3:\n",
        "                continue\n",
        "\n",
        "            entity_property, _, polarity = annotation\n",
        "\n",
        "            if polarity == '------------':\n",
        "                continue\n",
        "\n",
        "            if entity_property == pair:\n",
        "                entity_data['input_ids'].append(input_ids)\n",
        "                entity_data['attention_mask'].append(attention_mask)\n",
        "                entity_data['label'].append(tf_name_to_id['True'])\n",
        "\n",
        "                polarity_id = polarity_name_to_id.get(polarity)\n",
        "                if polarity_id is not None:\n",
        "                    polarity_data['input_ids'].append(input_ids)\n",
        "                    polarity_data['attention_mask'].append(attention_mask)\n",
        "                    polarity_data['label'].append(polarity_id)\n",
        "\n",
        "                matched = True\n",
        "                break\n",
        "\n",
        "        if not matched:\n",
        "            entity_data['input_ids'].append(input_ids)\n",
        "            entity_data['attention_mask'].append(attention_mask)\n",
        "            entity_data['label'].append(tf_name_to_id['False'])\n",
        "\n",
        "    return entity_data, polarity_data\n",
        "\n",
        "def get_dataset(raw_data, tokenizer, max_len):\n",
        "    entity_inputs, entity_masks, entity_labels = [], [], []\n",
        "    polarity_inputs, polarity_masks, polarity_labels = [], [], []\n",
        "\n",
        "    for utterance in raw_data:\n",
        "        form = utterance.get('sentence_form', '')\n",
        "        form = preprocessing(form)\n",
        "        if len(form) < 10:\n",
        "            continue\n",
        "        annotations = utterance.get('annotation', [])\n",
        "\n",
        "        entity_dict, polarity_dict = tokenize_and_align_labels(tokenizer, form, annotations, max_len)\n",
        "\n",
        "        entity_inputs.extend(entity_dict['input_ids'])\n",
        "        entity_masks.extend(entity_dict['attention_mask'])\n",
        "        entity_labels.extend(entity_dict['label'])\n",
        "\n",
        "        polarity_inputs.extend(polarity_dict['input_ids'])\n",
        "        polarity_masks.extend(polarity_dict['attention_mask'])\n",
        "        polarity_labels.extend(polarity_dict['label'])\n",
        "\n",
        "    def compute_class_weight(labels, label_size):\n",
        "        counter = Counter(labels)\n",
        "        total = sum(counter.values())\n",
        "        return torch.tensor([\n",
        "            total / counter.get(i, 1) if counter.get(i, 0) > 0 else 0.0\n",
        "            for i in range(label_size)\n",
        "        ], dtype=torch.float)\n",
        "\n",
        "    entity_dataset = TensorDataset(\n",
        "        torch.tensor(entity_inputs, dtype=torch.long),\n",
        "        torch.tensor(entity_masks, dtype=torch.long),\n",
        "        torch.tensor(entity_labels, dtype=torch.long)\n",
        "    )\n",
        "\n",
        "    polarity_dataset = TensorDataset(\n",
        "        torch.tensor(polarity_inputs, dtype=torch.long),\n",
        "        torch.tensor(polarity_masks, dtype=torch.long),\n",
        "        torch.tensor(polarity_labels, dtype=torch.long)\n",
        "    )\n",
        "\n",
        "    entity_weights = compute_class_weight(entity_labels, len(tf_name_to_id))\n",
        "    polarity_weights = compute_class_weight(polarity_labels, len(polarity_name_to_id))\n",
        "\n",
        "    return entity_dataset, polarity_dataset, entity_weights, polarity_weights"
      ],
      "metadata": {
        "id": "FBEFNwIep8LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from tqdm import trange\n",
        "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "def evaluation(y_true, y_pred, label_len):\n",
        "    count_list = [0] * label_len\n",
        "    hit_list = [0] * label_len\n",
        "\n",
        "    for i in range(len(y_true)):\n",
        "        count_list[y_true[i]] += 1\n",
        "        if y_true[i] == y_pred[i]:\n",
        "            hit_list[y_true[i]] += 1\n",
        "\n",
        "    acc_list = [hit / count if count > 0 else 0 for hit, count in zip(hit_list, count_list)]\n",
        "    print(f'Accuracy: {sum(hit_list) / sum(count_list):.4f}')\n",
        "    print(f'Macro Accuracy: {sum(acc_list) / label_len:.4f}')\n",
        "    print('F1 (per class):', f1_score(y_true, y_pred, average=None))\n",
        "    print('F1 Micro:', f1_score(y_true, y_pred, average='micro'))\n",
        "    print('F1 Macro:', f1_score(y_true, y_pred, average='macro'))\n",
        "\n",
        "scaler = GradScaler()\n",
        "def train_one_epoch(model, dataloader, optimizer, scheduler, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            loss, logits = model(input_ids, attention_mask, labels)\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask, label_ids = [t.to(device) for t in batch]\n",
        "            _, logits = model(input_ids, attention_mask)\n",
        "            pred = torch.argmax(logits, dim=-1)\n",
        "            preds.extend(pred.tolist())\n",
        "            labels.extend(label_ids.tolist())\n",
        "\n",
        "    macro_f1 = f1_score(labels, preds, average='macro')\n",
        "    return macro_f1, preds, labels\n",
        "\n",
        "def get_optimizer_scheduler(model, dataloader):\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(grouped_parameters, lr=learning_rate, eps=eps)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
        "                                                num_training_steps=len(dataloader) * num_train_epochs)\n",
        "    return optimizer, scheduler\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, mode='max'):\n",
        "        self.patience = patience\n",
        "        self.mode = mode\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.should_stop = False\n",
        "\n",
        "    def step(self, score):\n",
        "        if self.best_score is None or \\\n",
        "           (self.mode == 'max' and score > self.best_score) or \\\n",
        "           (self.mode == 'min' and score < self.best_score):\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.should_stop = True\n",
        "\n",
        "def train_sentiment_analysis():\n",
        "    print('train_sentiment_analysis START')\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    tokenizer.model_max_length = max_len\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    train_data = jsonlload(train_data_path)\n",
        "    dev_data = jsonlload(dev_data_path)\n",
        "\n",
        "    entity_train, polarity_train, entity_weights, polarity_weights = get_dataset(train_data, tokenizer, max_len)\n",
        "    entity_dev, polarity_dev, _, _ = get_dataset(dev_data, tokenizer, max_len)\n",
        "\n",
        "    entity_train_loader = DataLoader(entity_train, shuffle=True, batch_size=batch_size)\n",
        "    entity_dev_loader = DataLoader(entity_dev, shuffle=False, batch_size=batch_size)\n",
        "    polarity_train_loader = DataLoader(polarity_train, shuffle=True, batch_size=batch_size)\n",
        "    polarity_dev_loader = DataLoader(polarity_dev, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "    entity_model = ElectraBaseClassifier(base_model, len(tf_name_to_id), len(tokenizer)).to(device)\n",
        "    polarity_model = ElectraBaseClassifier(base_model, len(polarity_name_to_id), len(tokenizer)).to(device)\n",
        "\n",
        "    entity_loss_fn = torch.nn.CrossEntropyLoss(weight=entity_weights.to(device))\n",
        "    polarity_loss_fn = torch.nn.CrossEntropyLoss(weight=polarity_weights.to(device))\n",
        "\n",
        "    entity_opt, entity_sched = get_optimizer_scheduler(entity_model, entity_train_loader)\n",
        "    polarity_opt, polarity_sched = get_optimizer_scheduler(polarity_model, polarity_train_loader)\n",
        "\n",
        "    early_stop_entity = EarlyStopping(patience=3, mode='max')\n",
        "    early_stop_polarity = EarlyStopping(patience=3, mode='max')\n",
        "\n",
        "    for epoch in trange(num_train_epochs, desc=\"Epoch\"):\n",
        "        entity_loss = train_one_epoch(entity_model, entity_train_loader, entity_opt, entity_sched, entity_loss_fn)\n",
        "        print(f\"[Entity] Epoch {epoch+1} | Train Loss: {entity_loss:.4f}\")\n",
        "\n",
        "        if do_eval:\n",
        "            f1, preds, labels = evaluate_model(entity_model, entity_dev_loader)\n",
        "            print(f\"[Entity] Dev F1_macro: {f1:.4f}\")\n",
        "            if f1 > (early_stop_entity.best_score or 0):\n",
        "                torch.save(entity_model.state_dict(), os.path.join(ACD_MODEL_DIR, 'best_model.pt'))\n",
        "                print(\"Saved best entity model\")\n",
        "            early_stop_entity.step(f1)\n",
        "            if early_stop_entity.should_stop:\n",
        "                print(\"Early stopping triggered (Entity)\")\n",
        "                break\n",
        "\n",
        "        polarity_loss = train_one_epoch(polarity_model, polarity_train_loader, polarity_opt, polarity_sched, polarity_loss_fn)\n",
        "        print(f\"[Polarity] Epoch {epoch+1} | Train Loss: {polarity_loss:.4f}\")\n",
        "\n",
        "        if do_eval:\n",
        "            f1, preds, labels = evaluate_model(polarity_model, polarity_dev_loader)\n",
        "            print(f\"[Polarity] Dev F1_macro: {f1:.4f}\")\n",
        "            if f1 > (early_stop_polarity.best_score or 0):\n",
        "                torch.save(polarity_model.state_dict(), os.path.join(ASC_MODEL_DIR, 'best_model.pt'))\n",
        "                print(\"Saved best polarity model\")\n",
        "            early_stop_polarity.step(f1)\n",
        "            if early_stop_polarity.should_stop:\n",
        "                print(\"Early stopping triggered (Polarity)\")\n",
        "                break\n",
        "\n",
        "    print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "gOx_VuGgp_Qv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1748999207504,
          "user_tz": -540,
          "elapsed": 2,
          "user": {
            "displayName": "melon",
            "userId": "16479606032837208631"
          }
        },
        "outputId": "eab9ad17-3f66-42dd-c9bb-c5b87fd93408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-44b5c53f1c74>:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_jsonl_file(\n",
        "    jsonl_path='/content/drive/MyDrive/ABSA/data/converted_rawdata_final.jsonl',\n",
        "    output_dir='/content/drive/MyDrive/ABSA/data/',\n",
        "    train_ratio=0.7,\n",
        "    dev_ratio=0.15,\n",
        "    test_ratio=0.15\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixhEQ5ZQlI9Q",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1748999207507,
          "user_tz": -540,
          "elapsed": 2,
          "user": {
            "displayName": "melon",
            "userId": "16479606032837208631"
          }
        },
        "outputId": "e5afa624-fc9a-442a-ad63-1b2cdd072168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 분할 완료: train=643, dev=138, test=138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentiment_analysis()"
      ],
      "metadata": {
        "id": "VfQLsepJqC9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d30691aa490546c8937a0350cd842e16",
            "6b396f0ab8be4c2e815e81d15e35b1dd",
            "33127225d6344f9ca58897fb57edcd5b",
            "497031df694748208b1965b043a6cc92",
            "39994fbc98ee4d34bb826522c3f01e56",
            "a223bbbf93754f55868c1060448f3f74",
            "a701870cfa74479eaf3790add3ff915e",
            "397d5c3cbeec4f6aa2f7daafa1918e58",
            "f02edb901ace4659a2ceaeb9ff69c449",
            "132b1eea510445a1b0979d10e89aebad",
            "e40be7f58c0845c5887a7ee84140470b",
            "737ab7ea23b249b8b3d349055dadfc36",
            "39e77ba3b5e5425aa04cb029a2690520",
            "77f19941ab7f4768adcadee36da8e61c",
            "f370300d2e104a5ba7cc1dc31d8d2a6e",
            "621a90de46ad41c4968b655e4f8585b6",
            "8f54608c1c364477bbaa59618e290134",
            "31c8277a56d4413a83c30625fbb00a41",
            "76d3733785024fbea111aa9374307a82",
            "169d1d69e357444a9b0226817c213a76",
            "aaecb59ff15d49c08a514f9655b54018",
            "e0b9eb5a66084004914e39cb58214db0"
          ]
        },
        "outputId": "6a36a370-5eed-4f04-fe26-989eed57e896",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1749000756476,
          "user_tz": -540,
          "elapsed": 1548969,
          "user": {
            "displayName": "melon",
            "userId": "16479606032837208631"
          }
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_sentiment_analysis START\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/473M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d30691aa490546c8937a0350cd842e16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/473M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "737ab7ea23b249b8b3d349055dadfc36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Entity] Epoch 1 | Train Loss: 0.7095\n",
            "[Entity] Dev F1_macro: 0.6398\n",
            "Saved best entity model\n",
            "[Polarity] Epoch 1 | Train Loss: 1.1137\n",
            "[Polarity] Dev F1_macro: 0.3096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   5%|▌         | 1/20 [03:12<1:00:51, 192.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best polarity model\n",
            "[Entity] Epoch 2 | Train Loss: 0.5591\n",
            "[Entity] Dev F1_macro: 0.8606\n",
            "Saved best entity model\n",
            "[Polarity] Epoch 2 | Train Loss: 1.1053\n",
            "[Polarity] Dev F1_macro: 0.3439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  10%|█         | 2/20 [06:22<57:15, 190.86s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best polarity model\n",
            "[Entity] Epoch 3 | Train Loss: 0.3798\n",
            "[Entity] Dev F1_macro: 0.8911\n",
            "Saved best entity model\n",
            "[Polarity] Epoch 3 | Train Loss: 1.0817\n",
            "[Polarity] Dev F1_macro: 0.3733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  15%|█▌        | 3/20 [09:32<54:03, 190.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best polarity model\n",
            "[Entity] Epoch 4 | Train Loss: 0.3302\n",
            "[Entity] Dev F1_macro: 0.8993\n",
            "Saved best entity model\n",
            "[Polarity] Epoch 4 | Train Loss: 1.0574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  20%|██        | 4/20 [12:39<50:28, 189.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Polarity] Dev F1_macro: 0.3692\n",
            "[Entity] Epoch 5 | Train Loss: 0.2952\n",
            "[Entity] Dev F1_macro: 0.9077\n",
            "Saved best entity model\n",
            "[Polarity] Epoch 5 | Train Loss: 1.0494\n",
            "[Polarity] Dev F1_macro: 0.4815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  25%|██▌       | 5/20 [15:48<47:15, 189.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best polarity model\n",
            "[Entity] Epoch 6 | Train Loss: 0.2684\n",
            "[Entity] Dev F1_macro: 0.9087\n",
            "Saved best entity model\n",
            "[Polarity] Epoch 6 | Train Loss: 1.0210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  30%|███       | 6/20 [18:55<43:58, 188.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Polarity] Dev F1_macro: 0.4749\n",
            "[Entity] Epoch 7 | Train Loss: 0.2595\n",
            "[Entity] Dev F1_macro: 0.9089\n",
            "Saved best entity model\n",
            "[Polarity] Epoch 7 | Train Loss: 1.0102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  35%|███▌      | 7/20 [22:12<41:25, 191.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Polarity] Dev F1_macro: 0.4426\n",
            "[Entity] Epoch 8 | Train Loss: 0.2339\n",
            "[Entity] Dev F1_macro: 0.9087\n",
            "[Polarity] Epoch 8 | Train Loss: 0.9446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  35%|███▌      | 7/20 [25:17<46:58, 216.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Polarity] Dev F1_macro: 0.4553\n",
            "Early stopping triggered (Polarity)\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_from_korean_form(tokenizer, ce_model, pc_model, data, max_len=256):\n",
        "    ce_model.eval()\n",
        "    pc_model.eval()\n",
        "\n",
        "    for sentence in data:\n",
        "        form = sentence.get('sentence_form', '')\n",
        "        sentence['annotation'] = []\n",
        "\n",
        "        if not isinstance(form, str) or not form.strip():\n",
        "            print(f\"Invalid sentence skipped: {form}\")\n",
        "            continue\n",
        "\n",
        "        for pair in entity_property_pair:\n",
        "            encoded = tokenizer(\n",
        "                form,\n",
        "                pair,\n",
        "                padding='max_length',\n",
        "                max_length=max_len,\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            input_ids = encoded['input_ids'].to(device)\n",
        "            attention_mask = encoded['attention_mask'].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, ce_logits = ce_model(input_ids, attention_mask)\n",
        "            ce_pred = torch.argmax(ce_logits, dim=-1).item()\n",
        "\n",
        "            if tf_id_to_name[ce_pred] == 'True':\n",
        "                with torch.no_grad():\n",
        "                    _, pc_logits = pc_model(input_ids, attention_mask)\n",
        "                pc_pred = torch.argmax(pc_logits, dim=-1).item()\n",
        "\n",
        "                if 0 <= pc_pred < len(polarity_id_to_name):\n",
        "                    polarity = polarity_id_to_name[pc_pred]\n",
        "                else:\n",
        "                    polarity = \"UNKNOWN\"\n",
        "\n",
        "                sentence['annotation'].append([\n",
        "                    pair,\n",
        "                    [None, 0, 0],\n",
        "                    polarity\n",
        "                ])\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "jJm9KywxqFr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation_f1(true_data, pred_data):\n",
        "    ce_eval = {'tp': 0, 'fp': 0, 'fn': 0}\n",
        "    pipeline_eval = {'tp': 0, 'fp': 0, 'fn': 0}\n",
        "\n",
        "    if len(true_data) != len(pred_data):\n",
        "        print(f\"Warning: Length mismatch (true={len(true_data)}, pred={len(pred_data)})\")\n",
        "\n",
        "    for true_item, pred_item in zip(true_data, pred_data):\n",
        "        true_annos = true_item.get('annotation', [])\n",
        "        pred_annos = pred_item.get('annotation', [])\n",
        "\n",
        "        true_ce_set = set()\n",
        "        true_pipeline_set = set()\n",
        "        for anno in true_annos:\n",
        "            if len(anno) >= 3:\n",
        "                true_ce_set.add(anno[0])\n",
        "                true_pipeline_set.add((anno[0], anno[2]))\n",
        "\n",
        "        pred_ce_set = set()\n",
        "        pred_pipeline_set = set()\n",
        "        for anno in pred_annos:\n",
        "            if len(anno) >= 3:\n",
        "                pred_ce_set.add(anno[0])\n",
        "                pred_pipeline_set.add((anno[0], anno[2]))\n",
        "\n",
        "        ce_eval['tp'] += len(true_ce_set & pred_ce_set)\n",
        "        ce_eval['fp'] += len(pred_ce_set - true_ce_set)\n",
        "        ce_eval['fn'] += len(true_ce_set - pred_ce_set)\n",
        "\n",
        "        pipeline_eval['tp'] += len(true_pipeline_set & pred_pipeline_set)\n",
        "        pipeline_eval['fp'] += len(pred_pipeline_set - true_pipeline_set)\n",
        "        pipeline_eval['fn'] += len(true_pipeline_set - pred_pipeline_set)\n",
        "\n",
        "    def calc_f1(tp, fp, fn):\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "        return {\n",
        "            'Precision': round(precision, 4),\n",
        "            'Recall': round(recall, 4),\n",
        "            'F1': round(f1, 4)\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        'category extraction result': calc_f1(**ce_eval),\n",
        "        'entire pipeline result': calc_f1(**pipeline_eval)\n",
        "    }"
      ],
      "metadata": {
        "id": "rQDzhi_cqIOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import copy\n",
        "import os\n",
        "\n",
        "def load_model(model_class, path, model_name, label_size, tokenizer_len):\n",
        "    model = model_class(model_name, label_size, tokenizer_len)\n",
        "    model.load_state_dict(torch.load(path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def pretty_print_result(result_dict):\n",
        "    print(\"\\nF1 Evaluation Result:\")\n",
        "    for name, metrics in result_dict.items():\n",
        "        print(f\"\\n▶ {name}\")\n",
        "        for k, v in metrics.items():\n",
        "            print(f\"   {k}: {v:.4f}\")\n",
        "\n",
        "def test_sentiment_analysis(save_path=None):\n",
        "    print(\"Starting Sentiment Analysis Test...\")\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "        tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    except Exception as e:\n",
        "        print(f\"Tokenizer load error: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        test_data = jsonlload(test_data_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load test data: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        entity_test_data, polarity_test_data, _, _ = get_dataset(test_data, tokenizer, max_len)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to preprocess test data: {e}\")\n",
        "        return\n",
        "\n",
        "    entity_test_loader = DataLoader(entity_test_data, shuffle=False, batch_size=batch_size)\n",
        "    polarity_test_loader = DataLoader(polarity_test_data, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "    try:\n",
        "        ce_model_path = os.path.join(ACD_MODEL_DIR, 'best_model.pt')\n",
        "        pc_model_path = os.path.join(ASC_MODEL_DIR, 'best_model.pt')\n",
        "\n",
        "        ce_model = load_model(ElectraBaseClassifier, ce_model_path, base_model, len(tf_id_to_name), len(tokenizer))\n",
        "        pc_model = load_model(ElectraBaseClassifier, pc_model_path, base_model, len(polarity_id_to_name), len(tokenizer))\n",
        "    except Exception as e:\n",
        "        print(f\"Model load error: {e}\")\n",
        "        return\n",
        "\n",
        "    pred_data = predict_from_korean_form(tokenizer, ce_model, pc_model, copy.deepcopy(test_data))\n",
        "\n",
        "    result = evaluation_f1(test_data, pred_data)\n",
        "    pretty_print_result(result)\n",
        "\n",
        "    if save_path:\n",
        "        try:\n",
        "            jsondump(pred_data, save_path)\n",
        "            print(f\"Saved predictions to {save_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save predictions: {e}\")"
      ],
      "metadata": {
        "id": "JgzQdJvPqJCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentiment_analysis()"
      ],
      "metadata": {
        "id": "43USMUhvqMG4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1749001042751,
          "user_tz": -540,
          "elapsed": 30742,
          "user": {
            "displayName": "melon",
            "userId": "16479606032837208631"
          }
        },
        "outputId": "86872555-3151-4009-fe34-d56bac75371a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Sentiment Analysis Test...\n",
            "\n",
            "F1 Evaluation Result:\n",
            "\n",
            "▶ category extraction result\n",
            "   Precision: 0.8765\n",
            "   Recall: 0.7889\n",
            "   F1: 0.8304\n",
            "\n",
            "▶ entire pipeline result\n",
            "   Precision: 0.7078\n",
            "   Recall: 0.6232\n",
            "   F1: 0.6628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F91GucB_tygU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}